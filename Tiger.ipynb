{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiger Tutorial: Solving POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial outlines how to define a POMDP using the [POMDPs.jl](https://github.com/sisl/POMDPs.jl) interface. We will go through a simple problem simply known as the tiger problem (we will refer to it as the tiger POMDP). After defining the tiger POMDP, we will use QMDP and SARSOP to solve the POMDP. If you are new to working with this package, check out the [tutorial](http://nbviewer.ipython.org/github/sisl/POMDPs.jl/blob/master/examples/GridWorld.ipynb) on MDPs first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "You need to install a few modules in order to use this notebook. If you have all the modules below installed, great! If not run the following commands:\n",
    "\n",
    "```julia\n",
    "# install the POMDPs.jl interface\n",
    "Pkg.clone(\"https://github.com/sisl/POMDPs.jl.git\")\n",
    "\n",
    "# install the SARSOP wrapper\n",
    "Pkg.clone(\"https://github.com/sisl/SARSOP.jl\")\n",
    "# build SARSOP, it builds from source, so this may take some time\n",
    "Pkg.build(\"SARSOP\")\n",
    "\n",
    "# install the QMDP solver\n",
    "Pkg.clone(\"https://github.com/sisl/QMDP.jl\")\n",
    "\n",
    "# install two helper modules\n",
    "Pkg.clone(\"https://github.com/sisl/POMDPToolbox.jl\") # this provides implementations of discrete belief updating\n",
    "Pkg.clone(\"https://github.com/sisl/POMDPDistributions.jl\") # helps with sampling \n",
    "```\n",
    "\n",
    "If you already have all of the modules above, make sure you have the most recent versions. Many of these are still under heavy development, so update before starting by running\n",
    "\n",
    "```julia\n",
    "Pkg.update()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "In the tiger POMDP, the agent is tasked with escaping from a room. There are two doors leading out of the room. Behind one of the doors is a tiger, and behind the other is sweet, sweet freedom. If the agent opens the door and finds the tiger, it gets eaten (and receives a reward of -100). If the agent opens the other door, it escapes and receives a reward of 10. The agent can also listen. Listening gives a noisy measuremnt of which door the tiger is hiding behind. Listening gives the agent the correct location of the tiger 85% of the time. The agent receives a reward of -1 for listening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first import POMDPs.jl\n",
    "using POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDP \n",
    "A POMDP is defined by the tuple\n",
    "$$(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O).$$\n",
    "In addition to the familiar, state $\\mathcal{S}$ and action $\\mathcal{A}$ spaces, we must also define an observation space $\\mathcal{Z}$ and an observation function $O$. The POMDP problem definition may be similar to the one for MDP. For example, if you wanted to add state uncertaitniy to your problem, you can define the observation space, and observation function in addition to your previous MDP definition.\n",
    "\n",
    "Before defining the spaces for this problem, let's first deinfe the concrete type for the tiger POMDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP (constructor with 3 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type TigerPOMDP <: POMDP\n",
    "    r_listen::Float64 # reward for listening (default -1)\n",
    "    r_findtiger::Float64 # reward for finding the tiger (default -100)\n",
    "    r_escapetiger::Float64 # reward for escaping (default 10)\n",
    "    p_listen_correctly::Float64 # prob of correctly listening (default 0.85)\n",
    "    discount_factor::Float64 # discount\n",
    "end\n",
    "# default constructor\n",
    "function TigerPOMDP()\n",
    "    return TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters in the problem definition, but we can treat them all as constants. You can read more about the Tiger problem and POMDPs [here](http://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_tutorial.pdf#page=28). However, we created a default constructor that allows us to initialize the tiger POMDP by simply running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP(-1.0,-100.0,10.0,0.85,0.95)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp = TigerPOMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States\n",
    "We define a concrete type to represent the state of the tiger POMDP. The only thing we need to know to represent the problem is which door the tiger is hiding behind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type TigerState <: State\n",
    "    tigerleft::Bool\n",
    "end\n",
    "# initialization function\n",
    "POMDPs.create_state(::TigerPOMDP) = TigerState(true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the state is a binary value, we represent it as a boolean, but we could have represented it as an integer or any other sensible type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "There are three possible actions our agent can take: open the left door, open the right door, and listen. For clarity, we will represent these with symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type TigerAction <: Action\n",
    "    act::Symbol\n",
    "end\n",
    "# initialization function\n",
    "POMDPs.create_action(::TigerPOMDP) = TigerAction(:listen);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent our actions with the following symbols: open left (:openl), open right (:openr), and listen (:listen). For example, the action below represnts listening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerAction(:listen)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TigerAction(:listen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "There are two possible observations: the agent either hears the tiger behind the left door or behind the right door. We use a boolean to represent the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerObservation <: Observation\n",
    "    obsleft::Bool \n",
    "end\n",
    "# initialization function\n",
    "POMDPs.create_observation(::TigerPOMDP) = TigerObservation(true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "Let's define our state, action and observation spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Space\n",
    "There are only two states in the tiger POMDP: the tiger is either behind the left door or behind the right door. Our state space is simply an array of the states in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerStateSpace <: AbstractSpace\n",
    "    states::Vector{TigerState} \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the ```states``` and the ```domain``` functions. Recall, that the ```states``` function returns the state space for a given POMDP type, and the ```domain``` function returns an iterator for a given space. Here, the domain function returns an array of the two possible states in our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POMDPs.states(::TigerPOMDP) = TigerStateSpace([TigerState(true), TigerState(false)])\n",
    "POMDPs.domain(space::TigerStateSpace) = space.states;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the [tutorial](http://nbviewer.ipython.org/github/sisl/POMDPs.jl/blob/master/examples/GridWorld.ipynb) on MDPs, we also defined a ```rand!``` function that sampled the space. We do not need this function when using QMDP or SARSOP. However, if you wanted to use Monte Carlo solvers solvers like POMCP or DESPOT you would need a function that can sample your spaces. You should also create these sampling functions if you plan to simulate your policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "There are three actions in our problem. Once again, we represent the action space as an array of the actions in our problem. The ```actions``` and ```domain``` functions serve a similar purpose to the ```states``` and ```domain``` functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type TigerActionSpace <: AbstractSpace\n",
    "    actions::Vector{TigerAction} \n",
    "end\n",
    "# define actions function\n",
    "POMDPs.actions(::TigerPOMDP) = TigerActionSpace([TigerAction(:openl), TigerAction(:openr), TigerAction(:listen)]); # default\n",
    "POMDPs.actions(::TigerPOMDP, ::TigerState, acts::TigerActionSpace) = acts; # convenience (actions do not change in different states)\n",
    "# define domain function\n",
    "POMDPs.domain(space::TigerActionSpace) = space.actions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "The observation space looks similar to the state space. Recall that the state represents the truth about our system, while the observation is potentially untuthful information recieves about the state. In the tiger POMDP, our observation could give us a false representation of our state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerObservationSpace <: AbstractSpace\n",
    "    obs::Vector{TigerObservation} \n",
    "end\n",
    "# function returning observation space\n",
    "POMDPs.observations(::TigerPOMDP) = TigerObservationSpace([TigerObservation(true), TigerObservation(false)]);\n",
    "POMDPs.observations(::TigerPOMDP, s::TigerState, obs::TigerObservationSpace) = obs;\n",
    "# function returning an iterator over that space\n",
    "POMDPs.domain(space::TigerObservationSpace) = space.obs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the POMDP spaces, let's move on to defining the model functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition and Observation Distributions\n",
    "Before defining the model functions, we first need to create a distributions data-type. In general, our distributions should support sampling and have a ```pdf``` method. If you only want to get a policy from the SARSOP and QMDP solvers, you do not need to worry about implementing a sampling function. However, if you want to simulate the policy, you should implement these functions.\n",
    "\n",
    "Since the transition and observation distributions have identical form, we could just use a single type to serve the needs of both. This will not be the case in general, and we will define two seperate types (although similar looking) for clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transition distribution type\n",
    "type TigerTransitionDistribution <: AbstractDistribution\n",
    "    probs::Vector{Float64} \n",
    "end\n",
    "# transition distribution initializer\n",
    "POMDPs.create_transition_distribution(::TigerPOMDP) = TigerTransitionDistribution([0.5, 0.5])\n",
    "\n",
    "# observation distribution type\n",
    "type TigerObservationDistribution <: AbstractDistribution\n",
    "    probs::Vector{Float64} \n",
    "end\n",
    "# observation distribution initializer\n",
    "POMDPs.create_observation_distribution(::TigerPOMDP) = TigerObservationDistribution([0.5, 0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the ```pdf``` function. For a discrete problem, this function returns the probability mass of a given element (state or observation in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transition pdf\n",
    "function POMDPs.pdf(d::TigerTransitionDistribution, s::TigerState)\n",
    "    s.tigerleft ? (return d.probs[1]) : (return d.probs[2]) \n",
    "end;\n",
    "# obsevation pdf\n",
    "function POMDPs.pdf(d::TigerObservationDistribution, o::TigerObservation)\n",
    "    o.obsleft ? (return d.probs[1]) : (return d.probs[2])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the sampling functions. We will use [POMDPDistributions.jl](https://github.com/sisl/POMDPDistributions.jl) to help us implement the sampling function. However, you are free to use any approach you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using POMDPDistributions\n",
    "\n",
    "# sample from transition distribution\n",
    "function POMDPs.rand!(rng::AbstractRNG, s::TigerState, d::TigerTransitionDistribution)\n",
    "    # we use a categorical distribution, and this will usually be enough for a discrete problem\n",
    "    c = Categorical(d.probs) # this comes from POMDPDistributions\n",
    "    # sample an integer from c\n",
    "    sp = rand(rng, c) # this function is also from POMDPDistributions\n",
    "    # if sp is 1 then tiger is on the left\n",
    "    sp == 1 ? (s.tigerleft=true) : (s.tigerleft=false)\n",
    "    return s\n",
    "end\n",
    "\n",
    "# similar for smapling from the observation distribution\n",
    "function POMDPs.rand!(rng::AbstractRNG, o::TigerObservation, d::TigerObservationDistribution)\n",
    "    c = Categorical(d.probs) \n",
    "    op = rand(rng, c) \n",
    "    # if op is 1 then we hear tiger on the left\n",
    "    op == 1 ? (o.obsleft=true) : (o.obsleft=false)\n",
    "    return o\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we have to do for our distribution functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model\n",
    "Here we define the transition model for the tiger POMDP. The model itself is fairly simple. Our state is represented by the location of the tiger (left or right). The location of the tiger doesn't change when the agent listens. However, after the agent opens the door, it reaches a terminal state. That is the agent either escapes or gets eaten. To simplify our formulation, we simply reset the location of the tiger randomly. We could model this problem with a terminal state (i.e. one in which the agent no longer receives reward) as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the transition mode\n",
    "function POMDPs.transition(pomdp::TigerPOMDP, s::TigerState, a::TigerAction, d::TigerTransitionDistribution=create_transition_distribution(pomdp))\n",
    "    probs = d.probs\n",
    "    # if open a door reset the tiger probs\n",
    "    if a.act == :openl || a.act == :openr\n",
    "        fill!(probs, 0.5)\n",
    "    # if tiger is on the left, distribution = 1.0 over the first state\n",
    "    elseif s.tigerleft\n",
    "        probs[1] = 1.0\n",
    "        probs[2] = 0.0\n",
    "    # otherwise distribution = 1.0 over the second state\n",
    "    else\n",
    "        probs[1] = 0.0\n",
    "        probs[2] = 1.0\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model\n",
    "The reward model caputres the immediate objectives of the agent. It recieves a large negative reward for opening the door with the tiger behind it (-100), gets a positive reward for opening the other door (+10), and a small penalty for listening (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function POMDPs.reward(pomdp::TigerPOMDP, s::TigerState, a::TigerAction)\n",
    "    r = 0.0\n",
    "    # small penalty for listening\n",
    "    if a.act == :listen\n",
    "        r += pomdp.r_listen\n",
    "    end\n",
    "    if a.act == :openl\n",
    "        # find tiger behind left door\n",
    "        if s.tigerleft\n",
    "            r += pomdp.r_findtiger\n",
    "        # escape through left door\n",
    "        else\n",
    "            r += pomdp.r_escapetiger\n",
    "        end\n",
    "    end\n",
    "    if a.act == :openr\n",
    "        # escape through the right door\n",
    "        if s.tigerleft\n",
    "            r += pomdp.r_escapetiger\n",
    "            # find tiger behind the right door\n",
    "        else\n",
    "            r += pomdp.r_findtiger\n",
    "        end\n",
    "    end\n",
    "    return r\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Model\n",
    "The observation model captures the uncertaintiy in the agent's lsitening ability. When we listen, we receive a noisy measurement of the tiger's location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function POMDPs.observation(pomdp::TigerPOMDP, s::TigerState, a::TigerAction, d::TigerObservationDistribution=create_observation_distribution(pomdp))\n",
    "    probs = d.probs\n",
    "    p = pomdp.p_listen_correctly # probability of listening correctly\n",
    "    if a.act == :listen\n",
    "        # if tiger is behind left door\n",
    "        if s.tigerleft\n",
    "            probs[1] = p # correct prob\n",
    "            probs[2] = (1.0-p) # wring prob\n",
    "        # if tiger is behind right door\n",
    "        else\n",
    "            probs[1] = (1.0-p) # wrong prob\n",
    "            probs[2] = p # correct prob\n",
    "        end\n",
    "    # if don't listen uniform\n",
    "    else\n",
    "        fill!(probs, 0.5)\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscallenous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the ```discount``` function and the functions that return the size of our spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POMDPs.discount(pomdp::TigerPOMDP) = pomdp.discount_factor\n",
    "POMDPs.n_states(::TigerPOMDP) = 2\n",
    "POMDPs.n_actions(::TigerPOMDP) = 3\n",
    "POMDPs.n_observations(::TigerPOMDP) = 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beliefs\n",
    "If you are somewhat familiar with Julia defining all of the above may have been relaitvely simple. However, all POMDPs must be represented with a belief. Implementing beliefs and their updaters can be tricky. Luckily, we provide you with some nice tools to work with beliefs. Note that if you just want to use SASROP to solve for the alpha-vectors, and use your own belief updating scheme you do not need to implement the functions below.\n",
    "\n",
    "We will use the [POMDPToolbox](https://github.com/sisl/POMDPToolbox.jl) module which implements a discrete belief type and a belief udpater for it. You can create a ```DiscreteBelief``` type by passing in the size of your state space. This will create a belief with an initial uniform distribution over the states. To update the belief, you will need to create a ```DiscreteUpdater``` by passing your model POMDP into it. You can see how this is done in the simulation loop below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will use the POMDPToolbox module\n",
    "using POMDPToolbox\n",
    "\n",
    "# define a initialization function\n",
    "POMDPs.create_belief(::TigerPOMDP) = DiscreteBelief(2) # the belief is over our two states\n",
    "# initial belief is same as create\n",
    "POMDPs.initial_belief(::TigerPOMDP) = DiscreteBelief(2);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSOP Solver\n",
    "Let's play around with the [SARSOP.jl](https://github.com/sisl/SARSOP.jl) solver. The module we provide is a wrapper for the SARSOP backend. You can find more information about it [here](http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerActionSpace([TigerAction(:openl),TigerAction(:openr),TigerAction(:listen)])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using SARSOP # load the module\n",
    "# initialize our tiger POMDP\n",
    "pomdp = TigerPOMDP()\n",
    "display(actions(pomdp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a pomdpx file: tiger.pomdpx\n",
      "\n",
      "Loading the model ...\n",
      "  input file   : tiger.pomdpx\n",
      "  loading time : 0.00s \n",
      "\n",
      "SARSOP initializing ...\n",
      "  initialization time : 0.00s\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      " Time   |#Trial |#Backup |LBound    |UBound    |Precision  |#Alphas |#Beliefs  \n",
      "-------------------------------------------------------------------------------\n",
      " 0.003   0       0        -20        92.8206    112.821     3        1        \n",
      " 0.004   2       51       -6.2981    63.1396    69.4377     7        16       \n",
      " 0.005   4       103      0.149651   52.2764    52.1268     9        21       \n",
      " 0.006   6       151      6.19248    42.0546    35.8621     9        21       \n",
      " 0.007   8       200      10.3563    35.232     24.8757     12       21       \n",
      " 0.007   11      250      14.0433    29.5471    15.5037     6        21       \n",
      " 0.008   14      300      16.545     25.0926    8.54759     10       21       \n",
      " 0.009   17      350      18.2281    21.8163    3.5882      14       21       \n",
      " 0.01    18      400      18.7451    20.9384    2.19328     8        21       \n",
      " 0.011   21      465      19.1109    20.0218    0.910956    5        21       \n",
      " 0.011   22      500      19.2369    19.7071    0.470219    11       21       \n",
      " 0.012   24      550      19.3036    19.5405    0.236865    6        21       \n",
      " 0.013   25      600      19.3369    19.4574    0.120445    13       21       \n",
      " 0.014   27      669      19.3579    19.4049    0.0469305   5        21       \n",
      " 0.015   28      713      19.3643    19.389     0.024739    5        21       \n",
      " 0.016   29      757      19.3676    19.3807    0.0130409   5        21       \n",
      " 0.016   30      801      19.3694    19.3763    0.0068744   5        21       \n",
      " 0.017   31      850      19.3704    19.3739    0.00351433  10       21       \n",
      " 0.018   32      900      19.3709    19.3725    0.00155165  5        21       \n",
      " 0.019   33      936      19.3711    19.3721    0.000976551 8        21       \n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "SARSOP finishing ...\n",
      "  target precision reached\n",
      "  target precision  : 0.001000\n",
      "  precision reached : 0.000977 \n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      " Time   |#Trial |#Backup |LBound    |UBound    |Precision  |#Alphas |#Beliefs  \n",
      "-------------------------------------------------------------------------------\n",
      " 0.019   33      936      19.3711    19.3721    0.000976551 5        21       \n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Writing out policy ...\n",
      "  output file : tiger.policy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POMDPAlphas(2x5 Array{Float64,2}:\n",
       " -81.5975   3.01448  24.6954    28.4025  19.3711\n",
       "  28.4025  24.6954    3.01452  -81.5975  19.3711,[0,2,2,1,2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what follows are functions provided by SARSOP\n",
    "policy = POMDPPolicy(\"tiger.policy\") # initialize the policy, the argument is the name you want for your policy file \n",
    "# create the .pomdpx file, this is the format which the SARSOP backend reads in\n",
    "pomdpfile = POMDPFile(pomdp, \"tiger.pomdpx\") # must end in .pomdpx\n",
    "# initialize the solver\n",
    "solver = SARSOPSolver()\n",
    "# run the solve function\n",
    "solve(solver, pomdpfile, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2x5 Array{Float64,2}:\n",
       " -81.5975   3.01448  24.6954    28.4025  19.3711\n",
       "  28.4025  24.6954    3.01452  -81.5975  19.3711"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can retrieve the alpha vectors by calling\n",
    "alphas(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now see how our policy changes with the belief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiscreteBelief([0.5,0.5],[0.5,0.5],2,true)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's initialize the beliefs\n",
    "b = initial_belief(pomdp) # the initial prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerAction(:listen)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai = action(policy, b) # index of action, you need to convert this to the true action, support for automatic conversion is coming soon\n",
    "# the index corresponds to the action in our action array\n",
    "action_map = domain(actions(pomdp)) # create a mapping array\n",
    "a = action_map[ai] # get the actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our uniform prior, the optimal policy is to listen. Let's now update our belief. To do that, we need to sample an observation from the system state. Recall that a belief update requires, an action, an observation, and the prior belief. Let's make a small simulation loop to see what happens to our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 1\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.85,0.15000000000000002]\n",
      "\n",
      "Time step 2\n",
      "Have belief: [0.85,0.15000000000000002], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.9697986577181208,0.030201342281879207]\n",
      "\n",
      "Time step 3\n",
      "Have belief: [0.9697986577181208,0.030201342281879207], taking action: TigerAction(:openr), got reward: 10.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Time step 4\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.15000000000000002,0.85]\n",
      "\n",
      "Time step 5\n",
      "Have belief: [0.15000000000000002,0.85], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.030201342281879207,0.9697986577181208]\n",
      "\n",
      "Time step 6\n",
      "Have belief: [0.030201342281879207,0.9697986577181208], taking action: TigerAction(:openl), got reward: 10.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Time step 7\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.85,0.15000000000000002]\n",
      "\n",
      "Time step 8\n",
      "Have belief: [0.85,0.15000000000000002], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Time step 9\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.85,0.15000000000000002]\n",
      "\n",
      "Time step 10\n",
      "Have belief: [0.85,0.15000000000000002], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Total reward: 12.0\n"
     ]
    }
   ],
   "source": [
    "s = create_state(pomdp)\n",
    "o = create_observation(pomdp)\n",
    "\n",
    "b = initial_belief(pomdp)\n",
    "\n",
    "updater = DiscreteUpdater(pomdp) # this comes from POMDPToolbox\n",
    "\n",
    "rng = MersenneTwister(9) # initialize a random number generator\n",
    "\n",
    "rtot = 0.0\n",
    "# lets run the simulation for ten time steps\n",
    "for i = 1:10\n",
    "    # get the action from our SARSOP policy\n",
    "    ai = action(policy, b)\n",
    "    a = action_map[ai]\n",
    "    # compute the reward\n",
    "    r = reward(pomdp, s, a)\n",
    "    rtot += r\n",
    "    \n",
    "    println(\"Time step $i\")\n",
    "    println(\"Have belief: $(b.b), taking action: $(a), got reward: $(r)\")\n",
    "    \n",
    "    # transition the system state\n",
    "    trans_dist = transition(pomdp, s, a)\n",
    "    rand!(rng, s, trans_dist)\n",
    "\n",
    "    # sample a new observation\n",
    "    obs_dist = observation(pomdp, s, a)\n",
    "    rand!(rng, o, obs_dist)\n",
    "    \n",
    "    # update the belief\n",
    "    b = update(updater, b, a, o)\n",
    "    \n",
    "    println(\"Saw observation: $(o), new belief: $(b.b)\\n\")\n",
    "\n",
    "end\n",
    "println(\"Total reward: $rtot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that over the first six time steps, the policy is fairly simple. We listen twice, and then decide which door to open. However, in the following steps, we get a mix of observations, which makes the decision harder. Our agent does not open a door, because its belief is still uniform at the last time step! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QMDP Solver\n",
    "We will briefly go over the [QMDP.jl](https://github.com/sisl/QMDP.jl) solver. You should use QMDP with a word of caution. QMDP assumes that all state uncetainty dissapears in the next time step. This could lead to bad policies in problems with information gathering actions. For example, in the tiger POMDP listening is an information gathering action, and the resulting QMDP policy is quite poor. However, QMDP can work very well in problems where the state uncertainity is not impacted by the agent's action (for example systems with noisy sensor measurements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1, residual: 14.75, iteration run-time: 1.2242e-5, total run-time: 1.2242e-5\n",
      "Iteration : 2, residual: 12.59046875, iteration run-time: 2.7326e-5, total run-time: 3.9568e-5\n",
      "Iteration : 3, residual: 11.564691406249999, iteration run-time: 9.425e-6, total run-time: 4.8993e-5\n",
      "Iteration : 4, residual: 10.943236428222654, iteration run-time: 5.533e-6, total run-time: 5.4526e-5\n",
      "Iteration : 5, residual: 10.2558588273941, iteration run-time: 7.527e-6, total run-time: 6.2053e-5\n",
      "Iteration : 6, residual: 9.587976314837448, iteration run-time: 3.434e-6, total run-time: 6.548699999999999e-5\n",
      "Iteration : 7, residual: 8.957886507199987, iteration run-time: 2.878e-6, total run-time: 6.836499999999999e-5\n",
      "Iteration : 8, residual: 8.367828168991792, iteration run-time: 5.664e-6, total run-time: 7.402899999999999e-5\n",
      "Iteration : 9, residual: 7.816304847983972, iteration run-time: 2.97e-6, total run-time: 7.699899999999999e-5\n",
      "Iteration : 10, residual: 7.301052156282395, iteration run-time: 5.631e-6, total run-time: 8.262999999999999e-5\n",
      "Iteration : 11, residual: 6.8197456599030915, iteration run-time: 4.748e-6, total run-time: 8.737799999999999e-5\n",
      "Iteration : 12, residual: 6.370163598662359, iteration run-time: 3.213e-6, total run-time: 9.059099999999999e-5\n",
      "Iteration : 13, residual: 5.9502184661618, iteration run-time: 4.366e-6, total run-time: 9.495699999999999e-5\n",
      "Iteration : 14, residual: 5.557957422333274, iteration run-time: 2.984e-6, total run-time: 9.794099999999999e-5\n",
      "Iteration : 15, residual: 5.191555653202798, iteration run-time: 3.113e-6, total run-time: 0.00010105399999999998\n",
      "Iteration : 16, residual: 4.849308471382599, iteration run-time: 5.819e-6, total run-time: 0.00010687299999999998\n",
      "Iteration : 17, residual: 4.529623527415254, iteration run-time: 2.798e-6, total run-time: 0.00010967099999999997\n",
      "Iteration : 18, residual: 4.231013435561891, iteration run-time: 4.472e-6, total run-time: 0.00011414299999999997\n",
      "Iteration : 19, residual: 3.952088861809358, iteration run-time: 5.11e-6, total run-time: 0.00011925299999999996\n",
      "Iteration : 20, residual: 3.6915520617660036, iteration run-time: 2.767e-6, total run-time: 0.00012201999999999996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QMDPPolicy(2x3 Array{Float64,2}:\n",
       "  37.6943  147.694   139.31 \n",
       " 149.448    41.1425  140.975,Action[TigerAction(:openl),TigerAction(:openr),TigerAction(:listen)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using QMDP\n",
    "\n",
    "# initialize the solver\n",
    "# key-word args are the maximum number of iterations the solver will run for, and the Bellman tolerance\n",
    "solver = QMDPSolver(max_iterations=20, tolerance=1e-3) \n",
    "\n",
    "# initialize the QMDP policy\n",
    "qmdp_policy = create_policy(solver, pomdp)\n",
    "\n",
    "# run the solver\n",
    "solve(solver, pomdp, qmdp_policy, verbose=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these alpha-vectors differ from those compute by SARSOP. Let's see how the policy looks in simulation. The simulation loop is the same as we saw above, but now we use the QMDP policy instead of the SARSOP one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step 1\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.85,0.15000000000000002]\n",
      "\n",
      "Time step 2\n",
      "Have belief: [0.85,0.15000000000000002], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.9697986577181208,0.030201342281879207]\n",
      "\n",
      "Time step 3\n",
      "Have belief: [0.9697986577181208,0.030201342281879207], taking action: TigerAction(:openr), got reward: 10.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Time step 4\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.15000000000000002,0.85]\n",
      "\n",
      "Time step 5\n",
      "Have belief: [0.15000000000000002,0.85], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.030201342281879207,0.9697986577181208]\n",
      "\n",
      "Time step 6\n",
      "Have belief: [0.030201342281879207,0.9697986577181208], taking action: TigerAction(:openl), got reward: 10.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Time step 7\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.85,0.15000000000000002]\n",
      "\n",
      "Time step 8\n",
      "Have belief: [0.85,0.15000000000000002], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Time step 9\n",
      "Have belief: [0.5,0.5], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(true), new belief: [0.85,0.15000000000000002]\n",
      "\n",
      "Time step 10\n",
      "Have belief: [0.85,0.15000000000000002], taking action: TigerAction(:listen), got reward: -1.0\n",
      "Saw observation: TigerObservation(false), new belief: [0.5,0.5]\n",
      "\n",
      "Total reward: 12.0\n"
     ]
    }
   ],
   "source": [
    "s = create_state(pomdp)\n",
    "o = create_observation(pomdp)\n",
    "\n",
    "b = initial_belief(pomdp)\n",
    "\n",
    "updater = DiscreteUpdater(pomdp) # this comes from POMDPToolbox\n",
    "\n",
    "rng = MersenneTwister(9) # initialize a random number generator\n",
    "\n",
    "rtot = 0.0\n",
    "# lets run the simulation for ten time steps\n",
    "for i = 1:10\n",
    "    # get the action from our SARSOP policy\n",
    "    a = action(qmdp_policy, b) # the QMDP action function returns the POMDP action not its index like the SARSOP action function\n",
    "    # compute the reward\n",
    "    r = reward(pomdp, s, a)\n",
    "    rtot += r\n",
    "    \n",
    "    println(\"Time step $i\")\n",
    "    println(\"Have belief: $(b.b), taking action: $(a), got reward: $(r)\")\n",
    "    \n",
    "    # transition the system state\n",
    "    trans_dist = transition(pomdp, s, a)\n",
    "    rand!(rng, s, trans_dist)\n",
    "\n",
    "    # sample a new observation\n",
    "    obs_dist = observation(pomdp, s, a)\n",
    "    rand!(rng, o, obs_dist)\n",
    "    \n",
    "    # update the belief\n",
    "    b = update(updater, b, a, o)\n",
    "    \n",
    "    println(\"Saw observation: $(o), new belief: $(b.b)\\n\")\n",
    "\n",
    "end\n",
    "println(\"Total reward: $rtot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are identical! At least for this short simulation. In general, if you are dealing with a complex problem, it is good to compare the SARSOP and QMDP policies. This framework makes comparing the two policies very simple once you have defined the problem! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.11",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
